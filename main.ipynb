{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcedcb12",
   "metadata": {},
   "source": [
    "# Assignment for Probabilistic Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1270201a",
   "metadata": {},
   "source": [
    "Tom OVISTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75254875",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e0001",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b07439",
   "metadata": {},
   "source": [
    "In order to apply the knowledge and skills acquired in the Probabilistic Machine Learning course, we will be building a bayesian (probabilistic) neural network for a classification problem.  \n",
    "\n",
    "\n",
    "Moreover, our field of research is speech enhancement, i.e., the task of removing background noise, reverberation and distortion from a target speech signal. Therefore, we will be focusing here on a related, albeit simpler, problem: the task of classification of different noise environments.\n",
    "\n",
    "\n",
    "For this task, we will be using audio signals from the [DEMAND dataset](https://www.kaggle.com/datasets/chrisfilo/demand), which contains several recordings taken in different noisy environments (e.g., a restaurant, a laundromat).\n",
    "\n",
    "\n",
    "In this Jupyter notebook, we apply some pre-processing to the data, then build a classification model using the [Pyro framework](https://pyro.ai/).\n",
    "\n",
    "A GitHub repository for this assignment is available [here](https://github.com/ovistetom/probabilistic-machine-learning-assignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057633c5",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d8794",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1f161",
   "metadata": {},
   "source": [
    "Remember to install the required libraries using the `requirements.sh` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import kagglehub\n",
    "import pyro\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2d70b",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e0640",
   "metadata": {},
   "source": [
    "## Download and Pre-Process Database (DEMAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91bada",
   "metadata": {},
   "source": [
    "We will use the [DEMAND dataset](https://www.kaggle.com/datasets/chrisfilo/demand) for this assignment.  \n",
    "Some pre-processing is required before we can use the dataset for training, such as splitting the audio files into chunks of a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r\".\\data\\demand\\versions\\1\"\n",
    "root_preprocessed = r\".\\data\\demand\\versions\\preprocessed\"\n",
    "os.makedirs(root, exist_ok=True)\n",
    "os.makedirs(root_preprocessed, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this to download dataset.\n",
    "if False: \n",
    "    path = kagglehub.dataset_download(\"chrisfilo/demand\")\n",
    "    print(f\"Initial path to dataset: {path}\")\n",
    "    shutil.move(src=path, dst=root)\n",
    "print(f\"Path to raw dataset: {root}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1debbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this to clean dataset (remove 48k files and move 16k files to root).\n",
    "if False:\n",
    "    for dir_name in os.listdir(root):\n",
    "        dir_path = os.path.join(root, dir_name)\n",
    "        if dir_name.endswith('48k'):\n",
    "            shutil.rmtree(dir_path)\n",
    "        elif dir_name.endswith('16k'):\n",
    "            dir_path_full = os.path.join(dir_path, dir_name[:-4])\n",
    "            shutil.move(src=dir_path_full, dst=root)\n",
    "            shutil.rmtree(dir_path)\n",
    "print(f\"Path to clean dataset: {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this to preprocess dataset.\n",
    "if False:\n",
    "    for dir_name in os.listdir(root):\n",
    "        c = 0\n",
    "        dir_path = os.path.join(root, dir_name)\n",
    "        if os.path.isdir(dir_path):\n",
    "            for file_name in os.listdir(dir_path):\n",
    "                # Open audio file.\n",
    "                file_path = os.path.join(dir_path, file_name)\n",
    "                waveform, sr = torchaudio.load(file_path, channels_first=True)\n",
    "                # Split waveform into chunks of length 5 seconds.\n",
    "                target_len = 5 * sr\n",
    "                fade_in_len = 512\n",
    "                fade_out_len = 512\n",
    "                fader = torchaudio.transforms.Fade(fade_in_len, fade_out_len)               \n",
    "                waveform_segments = torch.split(waveform, target_len, dim=-1)\n",
    "                waveform_segments = [s for s in waveform_segments if s.size(1)==target_len]\n",
    "                for segment in waveform_segments:\n",
    "                    # Apply fading and normalize.\n",
    "                    segment = fader(segment)\n",
    "                    segment = segment / segment.abs().max()\n",
    "                    # Save short audio segment.\n",
    "                    segment_name = f'{dir_name}_{c:0>4}.flac'\n",
    "                    segment_path = os.path.join(root_preprocessed, segment_name)\n",
    "                    torchaudio.save(uri=segment_path, src=segment, sample_rate=sr, channels_first=True)\n",
    "                    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this to split pre-processed dataset into Train, Validation and Test subsets.\n",
    "if False:\n",
    "    list_files = [f for f in os.listdir(root_preprocessed) if f.endswith('.flac')]\n",
    "    random.shuffle(list_files)\n",
    "    num_files = len(list_files)\n",
    "    trn_size = round(0.8*num_files)\n",
    "    tst_size = round(0.1*num_files)\n",
    "    val_size = round(0.1*num_files)\n",
    "    trn_files = list_files[:trn_size]\n",
    "    tst_files = list_files[trn_size:trn_size+tst_size]\n",
    "    val_files = list_files[-val_size:]\n",
    "    for subset, subset_name in [(trn_files, 'trn'), (tst_files, 'tst'), (val_files, 'val')]:\n",
    "        subset_path = os.path.join(root_preprocessed, subset_name)\n",
    "        os.makedirs(subset_path, exist_ok=True)\n",
    "        for file_name in subset:\n",
    "            file_path = os.path.join(root_preprocessed, file_name)\n",
    "            shutil.move(src=file_path, dst=subset_path)\n",
    "print(f\"Path to preprocessed dataset: {root_preprocessed}\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_CLASSES = {\n",
    "    'DKITCHEN': 0,\n",
    "    'DLIVING': 1,\n",
    "    'DWASHING': 2,\n",
    "    'NFIELD': 3,\n",
    "    'NPARK': 4,\n",
    "    'NRIVER': 5,\n",
    "    'OHALLWAY': 6,\n",
    "    'OMEETING': 7,\n",
    "    'OOFFICE': 8,\n",
    "    'PCAFETER': 9,\n",
    "    'PRESTO': 10,\n",
    "    'PSTATION': 11,\n",
    "    'SPSQUARE': 12,\n",
    "    'STRAFFIC': 13,\n",
    "    'TBUS': 14,\n",
    "    'TCAR': 15,\n",
    "    'TMETRO': 16,    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21512bb",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6188f",
   "metadata": {},
   "source": [
    "## Define DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1522cfc",
   "metadata": {},
   "source": [
    "Here, we define a PyTorch `DemandDataset` class which will allow us to parse through the (pre-processed) database more naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a5052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 root: str, \n",
    "                 subset: str, \n",
    "                 num_samples: int | None = None,\n",
    "    ):\n",
    "        \"\"\"Dataset class to handle the DEMAND database.\n",
    "\n",
    "        Args:\n",
    "            root (str): Path to the root directory of the dataset.\n",
    "            subset (str): Name of the subset to load.\n",
    "            num_samples (int, optional): Number of samples to load. Defaults to `None` (all samples). \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.subset_name = subset\n",
    "        self.subset_path = os.path.join(root, subset)\n",
    "        self.num_samples = num_samples\n",
    "        self.sample_names = self._collect_samples()\n",
    "    \n",
    "    def _collect_samples(self):\n",
    "        list_samples = [f for f in os.listdir(self.subset_path) if f.endswith('.flac')]\n",
    "        random.shuffle(list_samples)\n",
    "        return sorted(list_samples[:self.num_samples])\n",
    "\n",
    "    def _get_file_path(self, sample_name):\n",
    "        return os.path.join(self.subset_path, f'{sample_name}')\n",
    "\n",
    "    def _load_sample(self, n: int):\n",
    "        sample_name = self.sample_names[n]\n",
    "        sample_path = self._get_file_path(sample_name)\n",
    "        waveform, _ = torchaudio.load(sample_path, channels_first=True)\n",
    "        noise_type = self._get_noise_type(sample_name)\n",
    "        return waveform.squeeze(0), noise_type\n",
    "    \n",
    "    def _get_noise_type(self, sample_name):\n",
    "        return NOISE_CLASSES[sample_name.split('_')[0]]\n",
    "\n",
    "    def __getitem__(self, n: int):\n",
    "        \"\"\"Load the n-th sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            n (int): The index of the sample to be loaded.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the waveform and the corresponding noise type.\n",
    "        \"\"\"\n",
    "        return self._load_sample(n)  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec6f51",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b6b77",
   "metadata": {},
   "source": [
    "## Define Audio Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a131d",
   "metadata": {},
   "source": [
    "For simplification, and to reduce model complexity, we will not be using directly the waveforms from the database as input to our neural network.  \n",
    "Instead, we define a function that computes various audio features from the waveform using the [Librosa library](https://librosa.org/), and use these features as input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fcb5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_audio_features(batch_waveform, sr=16000):\n",
    "    batch_waveform_numpy = batch_waveform.numpy()\n",
    "    mfcc = torch.from_numpy(librosa.feature.mfcc(y=batch_waveform_numpy, sr=sr).mean(2))\n",
    "    spectral_kwargs = {'n_fft': 2**15, 'hop_length': 2**13}\n",
    "    temporal_kwargs = {'frame_length': 2**15, 'hop_length': 2**13}\n",
    "    spectral_centroid = torch.from_numpy(librosa.feature.spectral_centroid(y=batch_waveform_numpy, sr=sr, **spectral_kwargs))\n",
    "    spectral_bandwidth = torch.from_numpy(librosa.feature.spectral_bandwidth(y=batch_waveform_numpy, **spectral_kwargs))\n",
    "    spectral_contrast = torch.from_numpy(librosa.feature.spectral_contrast(y=batch_waveform_numpy, sr=sr).mean(2))\n",
    "    spectral_flatness = torch.from_numpy(librosa.feature.spectral_flatness(y=batch_waveform_numpy, **spectral_kwargs))\n",
    "    spectral_rolloff = torch.from_numpy(librosa.feature.spectral_rolloff(y=batch_waveform_numpy, sr=sr, **spectral_kwargs))\n",
    "    zero_crossing_rate = torch.from_numpy(librosa.feature.zero_crossing_rate(y=batch_waveform_numpy, **temporal_kwargs))\n",
    "    root_mean_square = torch.from_numpy(librosa.feature.rms(y=batch_waveform_numpy, **temporal_kwargs))\n",
    "    tempo = torch.from_numpy(librosa.feature.tempo(y=batch_waveform_numpy, sr=sr))\n",
    "    audio_features = torch.cat(\n",
    "        [mfcc,\n",
    "        spectral_centroid.squeeze(1),\n",
    "        spectral_bandwidth.squeeze(1),\n",
    "        spectral_contrast,\n",
    "        spectral_flatness.squeeze(1),\n",
    "        spectral_rolloff.squeeze(1),\n",
    "        zero_crossing_rate.squeeze(1),\n",
    "        root_mean_square.squeeze(1),\n",
    "        tempo],\n",
    "        dim=1,\n",
    "    )\n",
    "    return audio_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b81cf",
   "metadata": {},
   "source": [
    "Let's ensure that these audio features are appropriate to discriminate the different noise types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9571f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 500\n",
    "dataset_tst = DemandDataset(root=root_preprocessed, subset='tst', num_samples=NUM_SAMPLES)\n",
    "dataloader_tst = torch.utils.data.DataLoader(dataset_tst, batch_size=NUM_SAMPLES, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0027ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_waveform, labels in dataloader_tst:\n",
    "    spectral_features = compute_audio_features(batch_waveform, sr=16000)\n",
    "    spectral_rolloff = torch.from_numpy(librosa.feature.spectral_rolloff(y=batch_waveform.numpy(), sr=16000).mean(2)).squeeze(1)\n",
    "    zero_crossing_rate = torch.from_numpy(librosa.feature.zero_crossing_rate(y=batch_waveform.numpy()).mean(2)).squeeze(1)\n",
    "    break\n",
    "print(f\"{spectral_features.shape = }\")\n",
    "print(f\"{spectral_rolloff.shape = }\")\n",
    "print(f\"{zero_crossing_rate.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "axs.scatter(spectral_rolloff, zero_crossing_rate, c=labels, marker='.', cmap='tab20')\n",
    "axs.set_xlabel(\"Spectral Rolloff\")\n",
    "axs.set_ylabel(\"Zero Crossing Rate\")\n",
    "axs.grid()\n",
    "axs.set_title(\"Audio Features w/ Different Colors for Different Noise Types\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e1c08",
   "metadata": {},
   "source": [
    "Although not sufficient, the two plotted features are somewhat discriminative between the different classes.\n",
    "We can reasonably believe that with the additional features, our model will have sufficient information to separate the different noise types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770eb502",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f695d8",
   "metadata": {},
   "source": [
    "## Define Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29051c33",
   "metadata": {},
   "source": [
    "In this section, we define our bayesian neural network by leveraging `PyroModule` tools from Pyro (http://pyro.ai/examples/modules.html).  \n",
    "The proposed neural architecture is rather simple, consisting of stacked fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn.module import PyroModule, PyroSample\n",
    "import pyro.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNeuralNetwork(PyroModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # Define neural layers.\n",
    "        self.fc_in = PyroModule[torch.nn.Linear](in_features=input_size, out_features=hidden_size)\n",
    "        self.fc_hid = PyroModule[torch.nn.Linear](in_features=hidden_size, out_features=hidden_size)\n",
    "        self.fc_out = PyroModule[torch.nn.Linear](in_features=hidden_size, out_features=output_size)\n",
    "        # Define prior distributions of neural parameters.\n",
    "        self.fc_in.weight = PyroSample(prior=dist.Normal(0., 1.).expand([hidden_size, input_size]).to_event(2))\n",
    "        self.fc_in.bias = PyroSample(prior=dist.Normal(0., 1.).expand([hidden_size]).to_event(1))\n",
    "        self.fc_hid.weight = PyroSample(prior=dist.Normal(0., 1.).expand([hidden_size, hidden_size]).to_event(2))\n",
    "        self.fc_hid.bias = PyroSample(prior=dist.Normal(0., 1.).expand([hidden_size]).to_event(1))\n",
    "        self.fc_out.weight = PyroSample(prior=dist.Normal(0., 1.).expand([output_size, hidden_size]).to_event(2))\n",
    "        self.fc_out.bias = PyroSample(prior=dist.Normal(0., 1.).expand([output_size]).to_event(1))\n",
    "        # Define layer normalizations.\n",
    "        self.layer_norm_in = PyroModule[torch.nn.LayerNorm](hidden_size)\n",
    "        self.layer_norm_hid = PyroModule[torch.nn.LayerNorm](hidden_size)\n",
    "        self.layer_norm_out = PyroModule[torch.nn.LayerNorm](output_size)\n",
    "        # Define activation functions.\n",
    "        self.activation = PyroModule[torch.nn.ReLU]()\n",
    "        self.log_softmax = PyroModule[torch.nn.LogSoftmax](dim=1)        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.layer_norm_in(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc_hid(x)\n",
    "        x = self.layer_norm_hid(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc_out(x)\n",
    "        x = self.layer_norm_out(x)\n",
    "        x = self.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 88  # Number of audio features.\n",
    "HIDDEN_SIZE = 64  # Size of hidden layers.\n",
    "OUTPUT_SIZE = 17  # Number of output classes (i.e., different noise types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b162c3",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30699c",
   "metadata": {},
   "source": [
    "## Define Pyro Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05103f",
   "metadata": {},
   "source": [
    "Now, in order to perform inference with our bayesian neural network (i.e., take a sample), we need to wrap it in a probabilistic model with a defined likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyroModel(PyroModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.network = BayesianNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, output=None):\n",
    "        logits = self.network(input)\n",
    "        with pyro.plate(\"data\"):  #, size=len(input)):\n",
    "            return pyro.sample(\"obs\", dist.Categorical(logits=logits), obs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c66698",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyroModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "guide = pyro.infer.autoguide.AutoNormal(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12915ae4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f3656",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f98d1",
   "metadata": {},
   "source": [
    "We can now train the model.\n",
    "\n",
    "NB: Implementation is kept rather straightforward for simplicity.  \n",
    "For example, we could implement cross-validation or other advanced training methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe97049",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "NUM_SAMPLES = 10000\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df26f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "dataset_trn = DemandDataset(root=root_preprocessed, subset='trn')\n",
    "dataloader_trn = torch.utils.data.DataLoader(dataset_trn, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer and inference method.\n",
    "pyro.clear_param_store()\n",
    "optim = pyro.optim.Adam({'lr': LEARNING_RATE})\n",
    "svi = pyro.infer.SVI(model, guide, optim, loss=pyro.infer.Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b269c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # TODO: Change this to train the model.    \n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for batch_waveform, labels in dataloader_trn:\n",
    "            batch_features = compute_audio_features(batch_waveform).to(DEVICE).float()\n",
    "            labels = labels.to(DEVICE)\n",
    "            # Perform a training step.\n",
    "            loss = svi.step(batch_features, labels) / labels.numel()\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        print(f\"Epoch {epoch:02} - Loss: {epoch_loss}\")\n",
    "\n",
    "        # Save the model.\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb8652",
   "metadata": {},
   "source": [
    "Here is the output of the training loop:\n",
    "```\n",
    "Epoch 00 - Loss: 1767609.256354446\n",
    "Epoch 01 - Loss: 512494.62495819316\n",
    "Epoch 02 - Loss: 60607.16488494873\n",
    "Epoch 03 - Loss: 7810.326712163294\n",
    "Epoch 04 - Loss: 4633.628274218243\n",
    "Epoch 05 - Loss: 4408.207026608792\n",
    "Epoch 06 - Loss: 4373.623391977942\n",
    "Epoch 07 - Loss: 4371.596310106918\n",
    "Epoch 08 - Loss: 4386.552696482344\n",
    "Epoch 09 - Loss: 4391.185009002682\n",
    "```\n",
    "\n",
    "We can see that the loss tends to over time: the model seems to be learning properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fbf5f",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150baa01",
   "metadata": {},
   "source": [
    "## Evaluate the Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad041cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 200\n",
    "dataset_tst = DemandDataset(root=root_preprocessed, subset='tst', num_samples=NUM_SAMPLES)\n",
    "dataloader_tst = torch.utils.data.DataLoader(dataset_tst, batch_size=NUM_SAMPLES, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b267839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy of the model.\n",
    "for batch_waveform, labels in dataloader_tst:\n",
    "    batch_features = compute_audio_features(batch_waveform).to(DEVICE).float()\n",
    "    labels = labels.to(DEVICE)\n",
    "    logits = model(batch_features)\n",
    "    # Compute accuracy.\n",
    "    accuracy = torch.mean((logits == labels).float())\n",
    "    print(f\"Accuracy: {accuracy*100:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef53a2",
   "metadata": {},
   "source": [
    "Evaluating our model reveals that it does not perform any better than random guessing.  \n",
    "Indeed, accuracy is around `1/17 â‰ƒ 6%` on average.  \n",
    "\n",
    "This is rather disappointing. However, the loss is undoubtebly decreasing; therefore, the model must be learning something!  \n",
    "Unfortunately, despite our best efforts and after much research, we have not found the reason for this apparent incoherence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
